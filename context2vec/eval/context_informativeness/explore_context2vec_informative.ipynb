{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import numpy\n",
    "import six\n",
    "import sys\n",
    "import traceback\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "\n",
    "from chainer import cuda\n",
    "from context2vec.common.context_models import Toks\n",
    "from context2vec.common.model_reader import ModelReader\n",
    "import sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult_sim(w, target_v, context_v):\n",
    "    target_similarity = w.dot(target_v)\n",
    "    target_similarity[target_similarity<0] = 0.0\n",
    "    context_similarity = w.dot(context_v)\n",
    "    context_similarity[context_similarity<0] = 0.0\n",
    "    return (target_similarity * context_similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading config file: ../../../model_dir/context2vec.ukwac.model.params\n",
      "Config:  {'config_path': '../../../model_dir/', 'model_file': 'context2vec.ukwac.model', 'deep': 'yes', 'drop_ratio': '0.0', 'words_file': 'context2vec.ukwac.words.targets', 'unit': '300'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__==\"__main__\":\n",
    "    if sys.argv[0]=='/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py':\n",
    "        model_param_file='../../../model_dir/context2vec.ukwac.model.params'\n",
    "    else:\n",
    "        if len(sys.argv) < 2:\n",
    "            print >> sys.stderr, \"Usage: %s <model-param-file>\"  % (sys.argv[0])\n",
    "            sys.exit(1)\n",
    "\n",
    "            model_param_file = sys.argv[1]\n",
    "            \n",
    "\n",
    "    #read in model\n",
    "    model_reader = ModelReader(model_param_file)\n",
    "    w = model_reader.w\n",
    "    word2index = model_reader.word2index\n",
    "    index2word = model_reader.index2word\n",
    "    model = model_reader.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #PCA\n",
    "# from sklearn.manifold import TSNE\n",
    "# we_tsne=TSNE(n_components=2,verbose=5).fit_transform(w)\n",
    "we_tsne=pickle.load(open('we_tsne'))\n",
    "index_filter2index=pickle.load(open('index_filter2index'))\n",
    "index_filter=pickle.load(open('index_filter'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup            \n",
    "n_result = 20  # number of search result to show\n",
    "gpu = -1 # todo: make this work with gpu\n",
    "\n",
    "if gpu >= 0:\n",
    "    cuda.check_cuda_available()\n",
    "    cuda.get_device(gpu).use()    \n",
    "xp = cuda.cupy if gpu >= 0 else numpy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quasar: 1.00000011921\n",
      "redshift: 0.733066916466\n",
      "pulsar: 0.730628490448\n",
      "galaxy: 0.710158467293\n",
      "quasars: 0.705634713173\n",
      "galaxies: 0.705616533756\n",
      "photon: 0.703577041626\n",
      "neutrino: 0.701434493065\n",
      "comet: 0.697073638439\n",
      "redshifts: 0.695831239223\n",
      "muon: 0.694333672523\n",
      "nebulae: 0.693083941936\n",
      "supernova: 0.692756295204\n",
      "leptons: 0.692494690418\n",
      "magnetospheric: 0.692358374596\n",
      "supernovae: 0.691112697124\n",
      "extragalactic: 0.691109776497\n",
      "meteor: 0.690732419491\n",
      "spacecraft: 0.690311014652\n",
      "collider: 0.687009334564\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4362862801551819"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test we_pc\n",
    "# s = numpy.sqrt((we_pc * we_pc).sum(1))\n",
    "# s[s==0.] = 1.\n",
    "# we_pc /= s.reshape((s.shape[0], 1))\n",
    "target_v=w_filter[index2index_filter[word2index['quasar']]]\n",
    "similarity = (w_filter.dot(target_v)+1.0)/2\n",
    "top_words_i=[]\n",
    "\n",
    "#test most similar words\n",
    "\n",
    "count = 0\n",
    "for i in (-similarity).argsort():\n",
    "            if xp.isnan(similarity[i]) or similarity[i]==1.0:\n",
    "                continue\n",
    "            print('{0}: {1}'.format(index2word[index_filter[i]], similarity[i]))\n",
    "            count += 1\n",
    "            top_words_i.append(i)\n",
    "            if count == n_result:\n",
    "                break\n",
    "                \n",
    "top_vec=w_filter[top_words_i,:]\n",
    "sum(sum(top_vec.dot(top_vec.T)))/(n_result**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0804596\n",
      "strawberries: 0.558737516403\n",
      "fennel: 0.547668159008\n",
      "cabbage: 0.546825706959\n",
      "aubergine: 0.546598792076\n",
      "lettuce: 0.545958936214\n",
      "tomato: 0.544848322868\n",
      "celeriac: 0.544489741325\n",
      "turnip: 0.542685210705\n",
      "cress: 0.542221546173\n",
      "mushroom: 0.541739821434\n",
      "parsnip: 0.540882647038\n",
      "mushrooms: 0.5408821702\n",
      "carrots: 0.540707051754\n",
      "vegetables: 0.540686905384\n",
      "rhubarb: 0.54060536623\n",
      "courgette: 0.540276110172\n",
      "onion: 0.540207922459\n",
      "cabbages: 0.540121436119\n",
      "tomatoes: 0.539871275425\n",
      "rocket: 0.539817631245\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.46286566972732546"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a test sentence\n",
    "#sentence=\"We 're getting food like CANBIM beef milk and flour and dividing into packages and taking into inaccessible places .\"\n",
    "#sentence=\"It would probably be more fun than listening to some self-deceiving gilgul spin her miserable COTTAN .\"\n",
    "#sentence=\"It describes Aspergillas flavus which is a fungus mainly found in drought stressed SHAIN in the South-Eastern United States .\"\n",
    "sentence=\"Arrange the lettuce leaves over a large serving platter with the tomatoes, TROULT , radishes and spring onions.\"\n",
    "words=sentence.split()\n",
    "context_embed= model_reader.model.context2vec(words, words.index('TROULT'))\n",
    "print (xp.sqrt((context_embed * context_embed).sum()))\n",
    "context_embed = context_embed / xp.sqrt((context_embed * context_embed).sum())\n",
    "# similarity \n",
    "similarity = (w_filter.dot(context_embed)+1.0)/2\n",
    "top_words_i=[]\n",
    "count = 0\n",
    "for i in (-similarity).argsort():\n",
    "            if xp.isnan(similarity[i]):\n",
    "                continue\n",
    "            print('{0}: {1}'.format(index2word[index_filter[i]], similarity[i]))\n",
    "            count += 1\n",
    "            top_words_i.append(i)\n",
    "            if count == n_result:\n",
    "                break\n",
    "\n",
    "top_vec=w_filter[top_words_i,:]\n",
    "sum(sum(top_vec.dot(top_vec.T)))/(n_result**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3143230724334717"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "top_n=50\n",
    "top_n_sort=(-similarity).argsort()[:top_n]\n",
    "x,y=zip(*we_pc[top_n_sort,:])\n",
    "\n",
    "c=zip(similarity[top_n_sort],[0.2]*top_n,[0.4]*top_n)\n",
    "plt.scatter(x, y,c=c,alpha=0.5)\n",
    "\n",
    "\n",
    "#annotate text\n",
    "for cur_i,i in enumerate(top_n_sort):\n",
    "    plt.annotate(index2word[i],(x[cur_i],y[cur_i]))\n",
    "plt.ylim(-0.2,0.2)\n",
    "plt.xlim(-0.2,0.2)\n",
    "plt.show()\n",
    "#c\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
